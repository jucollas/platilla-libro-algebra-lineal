{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9abfe8e4-c068-411f-85f4-c342a1ccd4e3",
      "metadata": {},
      "source": [
        "# Stochastic Gradient Descent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf584e2",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# pip install packages that are not in Pyodide\n",
        "%pip install ipympl==0.9.3\n",
        "%pip install seaborn==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa271d02-4ab3-4773-8846-dd7520c66ec8",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": [
          "disable-download-page",
          "thebe-remove-input-init",
          "disable-execution-page"
        ]
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from matplotlib import cm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mude_tools import magicplotter\n",
        "from cycler import cycler\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib widget\n",
        "\n",
        "# Set the color scheme\n",
        "sns.set_theme()\n",
        "colors = [\n",
        "    \"#0076C2\",\n",
        "    \"#EC6842\",\n",
        "    \"#A50034\",\n",
        "    \"#009B77\",\n",
        "    \"#FFB81C\",\n",
        "    \"#E03C31\",\n",
        "    \"#6CC24A\",\n",
        "    \"#EF60A3\",\n",
        "    \"#0C2340\",\n",
        "    \"#00B8C8\",\n",
        "    \"#6F1D77\",\n",
        "]\n",
        "plt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f4d0fd-ec99-4e92-be60-69cceb55cf63",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "This page contains interactive python element: click {fa}`rocket` --> {guilabel}`Live Code` in the top right corner to activate it.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Xy_1cRy9AOU?si=a0SnL6hmAtQHNyOU\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98eb1dc1-c438-43e4-97df-3a4de2ee41cb",
      "metadata": {},
      "source": [
        "For now, we used the same dataset at once. In some situations, it might be beneficial or necessary to look at only a part of the dataset, e.g., when\n",
        "\n",
        "- $N$ is too large and computing $( \\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} )^{-1}$ becomes prohibitively expensive\n",
        "- the model is nonlinear in $\\mathbf{w}$, and $\\mathbf{w}_\\mathrm{ML}$ does not have a closed-form solution\n",
        "- the dataset is arriving sequentially (e.g., in real-time from a sensor)\n",
        "\n",
        "Instead of solving for $\\mathbf{w}$ directly, we could employ an iterative optimization strategy. Let's first take a look at the data part of the error function, and its gradient with respect to $\\mathbf{w}$:\n",
        "\n",
        "$$\n",
        "E_D = \\frac{1}{2N} \\sum_{n = 1}^{N} \\left( t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\right)^2 \\quad \\mathrm{with \\, gradient} \\quad \\nabla E_D = - \\frac{1}{N} \\sum_{n=1}^N \\left(t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\right) \\boldsymbol{\\phi}_n.\n",
        "$$\n",
        "\n",
        "The standard formulation does not include the division by the dataset size, the stepsize is purely regulated through the learning rate. However, normalizing the gradient with $N$ makes the influence of the learning rate more consistent when considering different datasets. With a standard gradient descent algorithm, the update rule for the weights is given by\n",
        "\n",
        "$$\n",
        "\\mathbf{w}^{(\\tau + 1)} = \\mathbf{w}^{(\\tau)} - \\eta \\nabla E_D\n",
        "$$\n",
        "\n",
        "with a fixed *learning rate* $\\eta$. The costs for the gradient computations are independent of the dataset size $N$, when only considering subset $\\mathcal{B}$ of our dataset with $N_{\\mathcal{B}}$ data points. If we pick a random subset $\\mathcal{B}$ for each iteration of the optimization scheme, we have derived the *stochastic gradient descent* algorithm. Together with its numerous variants, this algorithm forms the backbone of many machine learning techniques. Most deep learning libraries, such as [`TensorFlow`](https://www.tensorflow.org/) or [`PyTorch`](https://pytorch.org/) offer implementations of these algorithms.\n",
        "\n",
        "We looked at the unregularized model to introduce SGD, but the extension to the regularized model is straightforward. Remember, in this case, the objective function is given by\n",
        "\n",
        "$$\n",
        "E (\\mathbf{w}) = \\frac{1}{2N} \\sum_{n = 1}^{N} \\left( t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\right)^2 + \\frac{\\lambda}{2N} \\mathbf{w}^T \\mathbf{w},\n",
        "$$\n",
        "\n",
        "and its gradient with respect to $\\mathbf{w}$ reads\n",
        "\n",
        "$$\n",
        "\\nabla E = \\frac{1}{N} \\left( - \\sum_{n=1}^N \\left(t_n - \\mathbf{w}^T \\boldsymbol{\\phi}_n \\right) \\boldsymbol{\\phi}_n + \\lambda \\, \\mathbf{w} \\right).\n",
        "$$\n",
        "\n",
        "When looking at the expresssion in the outer bracket, it becomes clear there are **two competing terms**: the first term pulls the weights towards the data, and the second term pulls them towards zero. Looking at the gradient, you can also see why ridge regression often is referred to as weight decay. The larger the weights become, the stronger the regularization term pulls them towards zero. If the data would not support a value for a certain weight, its presence in the gradient will lead to the weight decaying at a rate proportional to its magnitude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50c500a2-376a-448e-b67f-40c1e86aa958",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# This function returns the gradient of the cost function\n",
        "def get_gradient(x, t, w, basis, lam=0.0, **kwargs):\n",
        "    # Get the variable matrix using the basis function phi\n",
        "    Phi = basis(x.reshape(-1), **kwargs)\n",
        "    t = t.reshape(-1)\n",
        "\n",
        "    return (-(t - w @ Phi.T) @ Phi + lam * w) / len(t)\n",
        "\n",
        "\n",
        "# this computes the rmse\n",
        "def get_rmse(t_1, t_2, scaler):\n",
        "    se = (\n",
        "        scaler.inverse_transform(t_1[:, None]) - scaler.inverse_transform(t_2[:, None])\n",
        "    ) ** 2\n",
        "    rmse = np.sqrt(sum(se) / len(t_1))\n",
        "    return rmse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "34bbcf4e-ccdd-4c53-8039-712b01ee2e16",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# set parameters\n",
        "N, N_train = 500, 20\n",
        "M_radial = 50  # number of radial basis functions\n",
        "l_radial = 0.20  # lenghtscale of radial basis functions\n",
        "eta = 0.01  # learning rate\n",
        "epochs = 1000  # number of epochs\n",
        "N_batch = 5  # size of the minibatch B\n",
        "lam = np.exp(0.05)  # Regularization parameter\n",
        "\n",
        "# select where to plot solution\n",
        "epoch_plot_marks = np.logspace(np.log10(5), np.log10(epochs), 10, dtype=int)\n",
        "\n",
        "# randomly init weights\n",
        "np.random.seed(6)\n",
        "w = (np.random.rand(M_radial) - 0.5) / M_radial\n",
        "w_reg = w.copy()\n",
        "\n",
        "# get colormap\n",
        "rgb = cm.get_cmap(\"summer\")(np.linspace(0, 1, 10))[:, :3]\n",
        "\n",
        "# generate, scale, and partition data\n",
        "x, t = f_data(N=N + N_train)\n",
        "x_plot = np.linspace(0, 2 * np.pi, 100)\n",
        "xscaler, tscaler = StandardScaler(), StandardScaler()\n",
        "x_sc, t_sc = xscaler.fit_transform(x[:, None]), tscaler.fit_transform(t[:, None])\n",
        "x_train, x_val, t_train, t_val = train_test_split(x_sc, t_sc, train_size=N_train)\n",
        "x_train, x_val, t_train, t_val = (\n",
        "    x_train.reshape(-1),\n",
        "    x_val.reshape(-1),\n",
        "    t_train.reshape(-1),\n",
        "    t_val.reshape(-1),\n",
        ")\n",
        "\n",
        "# get features\n",
        "Phi_plot = RadialBasisFunctions(\n",
        "    xscaler.transform(x_plot[:, None]).reshape(-1), M_radial=M_radial, l_radial=l_radial\n",
        ")\n",
        "Phi_train = RadialBasisFunctions(x_train, M_radial=M_radial, l_radial=l_radial)\n",
        "Phi_val = RadialBasisFunctions(x_val, M_radial=M_radial, l_radial=l_radial)\n",
        "\n",
        "# create figure\n",
        "fig, ax = plt.subplots(2, 2, figsize=(11, 8), sharey=\"row\")\n",
        "fig.canvas.toolbar_visible = False\n",
        "plt.subplots_adjust(wspace=0.1)\n",
        "ax[0, 0].plot(\n",
        "    xscaler.inverse_transform(x_train[:, None]),\n",
        "    tscaler.inverse_transform(t_train[:, None]),\n",
        "    \"x\",\n",
        ")\n",
        "ax[0, 1].plot(\n",
        "    xscaler.inverse_transform(x_train[:, None]),\n",
        "    tscaler.inverse_transform(t_train[:, None]),\n",
        "    \"x\",\n",
        ")\n",
        "ax[0, 0].plot(x_plot, f_truth(x_plot), \"k-\"), ax[0, 1].plot(\n",
        "    x_plot, f_truth(x_plot), \"k-\"\n",
        ")\n",
        "ax[0, 0].set_xlabel(\"x\"), ax[0, 1].set_xlabel(\"x\"), ax[0, 0].set_ylabel(\"t\")\n",
        "ax[1, 0].set_xlabel(\"epochs\"), ax[1, 1].set_xlabel(\"epochs\"), ax[1, 0].set_ylabel(\n",
        "    \"RMSE\"\n",
        ")\n",
        "ax[0, 0].set_ylim(-2, 2), ax[0, 1].set_ylim(-2, 2)\n",
        "ax[0, 0].set_title(\"Unregularized\"), ax[0, 1].set_title(\"Regularized\")\n",
        "\n",
        "# init rmse\n",
        "rmse_train, rmse_train_reg = np.zeros(epochs), np.zeros(epochs)\n",
        "rmse_val, rmse_val_reg = np.zeros(epochs), np.zeros(epochs)\n",
        "\n",
        "# loop over epochs\n",
        "for i in range(epochs):\n",
        "    perm = np.random.permutation(N_train)\n",
        "\n",
        "    # loop over batches\n",
        "    for j in range(int(N_train / N_batch)):\n",
        "        x_batch = x_train[perm[j * N_batch : (j + 1) * N_batch]]\n",
        "        t_batch = t_train[perm[j * N_batch : (j + 1) * N_batch]]\n",
        "        dEdw = get_gradient(\n",
        "            x_batch,\n",
        "            t_batch,\n",
        "            w,\n",
        "            RadialBasisFunctions,\n",
        "            M_radial=M_radial,\n",
        "            l_radial=l_radial,\n",
        "        )\n",
        "        dEdw_reg = get_gradient(\n",
        "            x_batch,\n",
        "            t_batch,\n",
        "            w_reg,\n",
        "            RadialBasisFunctions,\n",
        "            lam=lam,\n",
        "            M_radial=M_radial,\n",
        "            l_radial=l_radial,\n",
        "        )\n",
        "        w = w - eta * dEdw\n",
        "        w_reg = w_reg - eta * dEdw_reg\n",
        "\n",
        "    # get predictions\n",
        "    t_pred_train = w @ Phi_train.T\n",
        "    t_pred_train_reg = w_reg @ Phi_train.T\n",
        "    t_pred_val = w @ Phi_val.T\n",
        "    t_pred_val_reg = w_reg @ Phi_val.T\n",
        "\n",
        "    # compute rmse\n",
        "    rmse_train[i] = get_rmse(t_train, t_pred_train, tscaler)\n",
        "    rmse_train_reg[i] = get_rmse(t_train, t_pred_train_reg, tscaler)\n",
        "    rmse_val[i] = get_rmse(t_val, t_pred_val, tscaler)\n",
        "    rmse_val_reg[i] = get_rmse(t_val, t_pred_val_reg, tscaler)\n",
        "\n",
        "    # plot some of the iterations to see progression of our model\n",
        "    if (i + 1) in epoch_plot_marks:\n",
        "        t_plot = w @ Phi_plot.T\n",
        "        t_plot_reg = w_reg @ Phi_plot.T\n",
        "\n",
        "        color = rgb[np.argwhere(epoch_plot_marks == i + 1)[0]]\n",
        "        ax[0, 0].plot(\n",
        "            x_plot,\n",
        "            tscaler.inverse_transform(t_plot[:, None]).reshape(-1),\n",
        "            color=color,\n",
        "            label=r\"{}\".format(i + 1),\n",
        "        )\n",
        "        ax[0, 1].plot(\n",
        "            x_plot,\n",
        "            tscaler.inverse_transform(t_plot_reg[:, None]).reshape(-1),\n",
        "            color=color,\n",
        "            label=r\"{}\".format(i + 1),\n",
        "        )\n",
        "\n",
        "# put legend\n",
        "ax[0, 0].legend(loc=\"lower left\", ncol=2)\n",
        "ax[0, 0].get_legend().set_title(r\"epochs\")\n",
        "\n",
        "# plot rmse\n",
        "ax[1, 0].semilogx(np.arange(1, epochs + 1), rmse_train, label=r\"$RMSE_{train}$\")\n",
        "ax[1, 0].semilogx(np.arange(1, epochs + 1), rmse_val, label=r\"$RMSE_{val}$\")\n",
        "\n",
        "ax[1, 1].semilogx(np.arange(1, epochs + 1), rmse_train_reg, label=r\"$RMSE_{train}$\")\n",
        "ax[1, 1].semilogx(np.arange(1, epochs + 1), rmse_val_reg, label=r\"$RMSE_{val}$\")\n",
        "ax[1, 0].legend(), ax[1, 1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0a01f43",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig17.png\n",
        "Training process of two models with different levels of regularization, predictions coming from model snapshots at different epochs.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "937e9d1f-b4f8-45c7-9b48-7ce37ddbe590",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": []
      },
      "source": [
        "You can see in the top figure that our predictions seem to converge towards a particular shape. The remaining discrepancy between our final model and the true function $f$ is due to our general model bias, and the particular dataset we drew.\n",
        "\n",
        "You can see that our validation error increases at some point, indicating that overfitting might occur. Again, note how the training error cannot feel that, and just decreases monotonically. SGD with minibatches already has a slight regularizing effect. Other remedies include the L2-regularization technique discussed discusses previously, early stopping, or collecting more data.\n",
        "\n",
        "Finally, it should be noted that the step size of the SGD must be chosen carefully. Try out for yourself what happens when you choose very small or large stepsizes by adapting the learning rate. Even though this optimization problem is well-defined and has a global minimum, SGD is not guaranteed to converge to it. Luckily, the most popular variants such as [AdaGrad][1], [RMSProp][2], and [Adam][3] feature some form of adaptive stepsize control, improving convergence rate and robustness. One usually starts with a larger stepsize to approach the minimum quickly. After that, the stepsize is reduced continuously to reliably uncover the exact location of the extremum.\n",
        "\n",
        "[1]: https://jmlr.org/papers/volume12/duchi11a/duchi11a.pdf\n",
        "[2]: http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf\n",
        "[3]: https://arxiv.org/abs/1412.6980"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0921e008-ab5e-4e52-9226-251cf711814f",
      "metadata": {},
      "source": [
        "In the following animation you can see another example of extreme overfitting being observed during SGD iterations:\n",
        "\n",
        "<center><img src=\"https://surfdrive.surf.nl/files/index.php/s/4Q8RDBb6ZKRgRBs/download\" width=\"1000\"></center>\n",
        "\n",
        "Observing when the validation loss starts to increase is a useful sign of the onset of overfitting. Stopping the training process when this is detected is the core of so-called **early stopping** strategies implemented in popular machine learning packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a7febb2-ad81-4eb7-a1de-7eaa6c9eed1e",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d66a05ba",
      "metadata": {},
      "source": [
        "% START-CREDIT\n",
        "% source: machine_learning\n",
        "```{attributiongrey} Attribution\n",
        ":class: attribution\n",
        "This chapter is written by Iuri Rocha, Anne Poot, Joep Storm and Leon Riccius. {ref}`Find out more here <machine_learning_credit>`.\n",
        "```\n",
        "% END-CREDIT"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

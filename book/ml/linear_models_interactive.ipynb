{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "369719c6-96f2-44a1-8e4e-a95d4d7aab9c",
      "metadata": {},
      "source": [
        "# Linear Basis Function Models and Regularization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f042228e",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# pip install packages that are not in Pyodide\n",
        "%pip install ipympl==0.9.3\n",
        "%pip install seaborn==0.12.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa271d02-4ab3-4773-8846-dd7520c66ec8",
      "metadata": {
        "tags": [
          "disable-download-page",
          "auto-execute-page",
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from mude_tools import magicplotter\n",
        "from cycler import cycler\n",
        "import seaborn as sns\n",
        "\n",
        "%matplotlib widget\n",
        "\n",
        "# Set the color scheme\n",
        "sns.set_theme()\n",
        "colors = [\n",
        "    \"#0076C2\",\n",
        "    \"#EC6842\",\n",
        "    \"#A50034\",\n",
        "    \"#009B77\",\n",
        "    \"#FFB81C\",\n",
        "    \"#E03C31\",\n",
        "    \"#6CC24A\",\n",
        "    \"#EF60A3\",\n",
        "    \"#0C2340\",\n",
        "    \"#00B8C8\",\n",
        "    \"#6F1D77\",\n",
        "]\n",
        "plt.rcParams[\"axes.prop_cycle\"] = cycler(color=colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73f4d0fd-ec99-4e92-be60-69cceb55cf63",
      "metadata": {},
      "source": [
        "This page contains interactive python element: click {fa}`rocket` --> {guilabel}`Live Code` in the top right corner to activate it.\n",
        "\n",
        "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/FAyrV8P6Jx0?si=Ox8PTi-qp4t9r1lV\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\" allowfullscreen></iframe>\n",
        "\n",
        "## Linear Basic Function Models\n",
        "So far, we have been using a non-parametric model with k-nearest neighbors, meaning we needed access to the whole training dataset for each prediction. We will now focus on parametric models, namely linear models with basis functions. Parametric models are defined by a finite set of parameters calibrated in a training step. All we need for a prediction then are the parameter values. There is no longer a need to carry the whole dataset with us; the information used to make predictions is encoded in the model parameters. Once again, we will employ the simple sine function to demonstrate the concepts presented in this chapter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "630454c8-740b-4eb8-8e26-bf45e45dd6bf",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# The true function relating t to x\n",
        "def f_truth(x, freq=1, **kwargs):\n",
        "    # Return a sine with a frequency of f\n",
        "    return np.sin(x * freq)\n",
        "\n",
        "\n",
        "# The data generation function\n",
        "def f_data(epsilon=0.7, N=100, **kwargs):\n",
        "    # Apply a seed if one is given\n",
        "    if \"seed\" in kwargs:\n",
        "        np.random.seed(kwargs[\"seed\"])\n",
        "\n",
        "    # Get the minimum and maximum\n",
        "    xmin = kwargs.get(\"xmin\", 0)\n",
        "    xmax = kwargs.get(\"xmax\", 2 * np.pi)\n",
        "\n",
        "    # Generate N evenly spaced observation locations\n",
        "    x = np.linspace(xmin, xmax, N)\n",
        "\n",
        "    # Generate N noisy observations (1 at each location)\n",
        "    t = f_truth(x, **kwargs) + np.random.normal(0, epsilon, N)\n",
        "\n",
        "    # Return both the locations and the observations\n",
        "    return x, t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f56179a",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Get the observed data\n",
        "x, t = f_data()\n",
        "\n",
        "# Plot the data and the ground truth\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "fig.canvas.toolbar_visible = False\n",
        "ax.set_position([0.2, 0.1, 0.7, 0.8])\n",
        "plt.plot(x, f_truth(x), \"k-\", label=r\"Ground truth $f(x)$\")\n",
        "plt.plot(x, t, \"x\", label=r\"Noisy data $(x,t)$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"t\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce248661",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig0.png\n",
        "Ground truth vs noisy data\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bf07109-113f-4118-b5d3-9cea1a2c5ce1",
      "metadata": {},
      "source": [
        "### Linear model\n",
        "\n",
        "The key idea behind linear regression models is that they are linear in their parameters $\\mathbf{w}$. They might be linear in their inputs as well, although this does not necessarily need to case, as we will see later on in this chapter. The simplest approach is to model our target function $y(x)$ as a linear combination of the coordinates $x$:\n",
        "\n",
        "$$\n",
        "y(x,\\mathbf{w}) = w_0 + w_1 x_1\n",
        "$$\n",
        "\n",
        "In the one dimensional case, this is equivalent to fitting a straight line through our datapoints. The parameter $w_0$, also referred to as bias (**not to be confused with the model bias from the previous chapter**), determines the intercept, $w_1$ determines the slope. The introduction of a dummy input $x_0 = 1$ allows us to write the model in a more concise way:\n",
        "\n",
        "$$\n",
        "y(x,\\mathbf{w}) = w_0 x_0 + w_1 x_1 = \\mathbf{w}^T \\mathbf{x}\n",
        "$$\n",
        "\n",
        "We will use the least squares error function from the previous chapter to fit our model, but will first show how this choice is motivated by a Maximum likelihood approach."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bd473f0-0b5b-486f-b41b-f15c2ddd0126",
      "metadata": {},
      "source": [
        "### Maximum Likelihood Estimation\n",
        "\n",
        "Oftentimes, it is assumed that the target $t$ is given by a deterministic function $y(\\mathbf{x}, \\mathbf{w})$ with additive Gaussian noise so that\n",
        "\n",
        "$$\n",
        "t = y(\\mathbf{x}, \\mathbf w ) + \\epsilon \\hspace{0.6cm} \\mathrm{with} \\hspace{0.6cm} \\epsilon \\sim \\mathcal{N} (0,\\beta^{-1}),\n",
        "$$\n",
        "\n",
        "with precision $\\beta$ (which is defined as $1/\\sigma^2$). Note that this assumption is only justified in case of a unimodal conditional distribution for $t$. It can have grave influence on the model accuracy and its validity therefore must be carefully assessed.\n",
        "\n",
        "As seen in the previous chapter, for the square loss function, the optimal prediction for some value $\\mathbf{x}$ is given by the conditional mean of the target variable $t$. In this case, this conditional mean is given by:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[t|\\mathbf{x}] = \\int t p(t|\\mathbf{x}) \\mathrm{d}t = y(\\mathbf{x},\\mathbf{w}).\n",
        "$$\n",
        "\n",
        "Given a new input $\\mathbf{x}$, the distribution of $t$ that follows from our model is\n",
        "\n",
        "$$\n",
        "p (t | \\mathbf{x}, \\mathbf{w}, \\beta) = \\mathcal{N} (t | y (\\mathbf{x}, \\mathbf{w}), \\beta^{-1}).\n",
        "$$\n",
        "\n",
        "Consider now a dataset $\\mathcal{D}$ consisting of inputs $\\mathbf{X} = \\{ \\mathbf{x}_1, \\dots, \\mathbf{x}_n \\}$ and targets $\\mathbf{t} = \\{  t_1, \\dots, t_n \\}$. Assuming our datapoints are drawn independently from the same distribution (*i.i.d. assumption*), the likelihood of drawing this dataset from our model is\n",
        "\n",
        "$$\n",
        "p( \\mathcal{D}|\\mathbf{w}) =  \\prod_{n = 1}^N \\mathcal{N} (t_n | y (\\mathbf{x}_n, \\mathbf{w}), \\beta^{-1}), \n",
        "$$\n",
        "\n",
        "also referred to as the likelihood function. Taking the logarithm and expanding on the classic expression for a multivariate Gaussian distribution gives:\n",
        "\n",
        "$$\n",
        "\\mathrm{ln} \\, p( \\mathcal{D}|\\mathbf{w}) = \\sum_{n = 1}^{N} \\mathrm{ln} \\, \\mathcal{N} ( t_n | \\mathbf{w}^T \\mathbf{x}_n, \\beta^{-1}) = \\frac{N}{2} \\mathrm{ln} \\beta - \\frac{N}{2}\\mathrm{ln}(2 \\pi) - \\beta \\, \\underbrace{\\frac{1}{2} \\sum_{n = 1}^{N} ( t_n - \\mathbf{w}^T \\mathbf{x}_n)^2}_{E_\\mathcal{D}}\n",
        "$$\n",
        "\n",
        "where we can identify our square-error loss function in the last term. Note that the first two terms are constant for a given dataset and have no influence on the parameter setting $\\bar{\\mathbf{w}}$ that maximizes the likelihood. Those optimal parameters values $\\bar{\\mathbf{w}}$ can be obtained by setting the gradient of our loss function w.r.t. $\\mathbf{w}$ to zero and solving for $\\mathbf{w}$.\n",
        "\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}} E_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^N \\left(t_n - \\mathbf{w}^T \\mathbf{x}_n \\right) \\mathbf{x}_n  \\stackrel{!}{=} 0\n",
        "$$\n",
        "\n",
        "It is convenient to concatenate all inputs to a *design matrix* $\\mathbf{X} = [\\mathbf{x}_1^T, ..., \\mathbf{x}_N^T]^T$. Solving for $\\mathbf{w}$ gives\n",
        "\n",
        "$$\n",
        "\\bar{\\mathbf{w}} = \\left(\\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\mathbf{X}^T \\mathbf{t}\n",
        "$$\n",
        "\n",
        "which is the classical expression for a least-squares solution you have by now seen many times during the course. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c9cd083-df9a-421f-a3bf-f18003cbd516",
      "metadata": {},
      "source": [
        "### Data normalization\n",
        "\n",
        "Before we move on to fitting the model, we normalize our data. **This step is recommended for most machine learning techniques and is often even necessary.** A non-normalized dataset almost always leads to a numerically more challenging optimization problem. Part of model selection is to ensure that our basis functions show the desired behavior in the relevant parts of the domain. We center and rescale our data, a process referred to as standardization, to operate in the vicinity of the origin only. The standardized dataset $\\hat{\\mathcal{D}} = ( \\hat{x}, \\hat{t} )$ is obtained by subtracting the sample mean $\\mu$, and dividing by the sample standard deviation $\\sigma$ of the data:\n",
        "\n",
        "$$\n",
        "\\hat{x} = \\frac{x - \\mu_x}{\\sigma_x}  \\quad \\mathrm{and} \\quad \\hat{t} = \\frac{t - \\mu_t}{\\sigma_t}.\n",
        "$$\n",
        "\n",
        "Take a look below at the standardized and unstandardized data. Note that the standardization of the target $t$ has a marginal effect, as the sine function is already centered at 0 and almost shows a standard deviation of 1. A 4 by 4 square has been added to indicate the region from $-2 \\hat{\\sigma}$ to $2 \\hat{\\sigma}$. As you can see, all input and output variables fall roughly in this interval, and this property allows for more stability when applying numerical solvers to the problem.\n",
        "\n",
        "Note that **there is no strictly correct way to shift and scale input data**. Depending on the distribution of the data, a min-max scaling or a quantile scaling might lead to a better numerical setup. The dataset's structure needs to be assessed carefully to make an informed decision on normalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1776378-434a-4a3f-880b-446c9bfd77ad",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# generate data, instantiate scaler, and fit tranform\n",
        "np.random.seed(0)\n",
        "x, t = f_data(N=100)\n",
        "xscaler, tscaler = StandardScaler(), StandardScaler()\n",
        "x_norm, t_norm = xscaler.fit_transform(x[:, None]), tscaler.fit_transform(t[:, None])\n",
        "\n",
        "# plot\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "fig.canvas.toolbar_visible = False\n",
        "ax.set_position([0.2, 0.1, 0.7, 0.8])\n",
        "ax.plot(x, t, \"x\", label=\"unnormalized data\")\n",
        "ax.plot(x_norm, t_norm, \"x\", label=\"data after normalization\")\n",
        "\n",
        "# Create a Rectangle patch\n",
        "rect = patches.Rectangle((-2, -2), 4, 4, linewidth=1.0, edgecolor=\"k\", facecolor=\"none\")\n",
        "\n",
        "# Add the patch to the Axes\n",
        "ax.add_patch(rect)\n",
        "ax.set_aspect(\"equal\", \"datalim\")\n",
        "\n",
        "plt.legend(loc=\"upper right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0bc39b0",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig3.png\n",
        "Unnormalized and normalized data\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58856941",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# Function for the basis functions\n",
        "def LinearBasis(x, **kwargs):\n",
        "    \"\"\"\n",
        "    Represents a 1D linear basis.\n",
        "    \"\"\"\n",
        "    num_basis = 2  # The number of basis functions is 2 due to the dummy input\n",
        "    x = x.reshape(-1, 1)\n",
        "    return np.hstack((np.ones_like(x), x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c8efa77",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# Define the prediction locations\n",
        "# (note that these are different from the locations where we observed our data)\n",
        "# x_pred = np.linspace(-1, 2*np.pi+1, 1000)\n",
        "\n",
        "\n",
        "# Define a function that makes a prediction at the given locations, based on the given (x,t) data\n",
        "def predict(x, t, x_pred, basis, normalize=True, **kwargs):\n",
        "    # reshape if necessary for scalers\n",
        "    x = x[:, None] if len(x.shape) == 1 else x\n",
        "    t = t[:, None] if len(t.shape) == 1 else t\n",
        "    x_pred = x_pred[:, None] if len(x_pred.shape) == 1 else x_pred\n",
        "\n",
        "    # normalize data (you will see why we have to do this further below)\n",
        "    xscaler, tscaler = StandardScaler(), StandardScaler()\n",
        "    if normalize == True:\n",
        "        x_sc, t_sc = xscaler.fit_transform(x), tscaler.fit_transform(t)\n",
        "    else:\n",
        "        x_sc, t_sc = x, t\n",
        "\n",
        "    # Get the variable matrix using the basis function phi\n",
        "    Phi = basis(x_sc.reshape(-1), **kwargs)\n",
        "    t_sc = t_sc.reshape(-1)\n",
        "    if normalize == True:\n",
        "        x_pred = xscaler.transform(x_pred).reshape(-1)\n",
        "    else:\n",
        "        x_pred = x_pred.reshape(-1)\n",
        "\n",
        "    # Get the coefficient vector\n",
        "    w = np.linalg.solve(Phi.T @ Phi, Phi.T @ t_sc)\n",
        "\n",
        "    # Make a prediction in the prediction locations\n",
        "    Phi_pred = basis(x_pred, **kwargs)\n",
        "    t_pred = Phi_pred @ w\n",
        "\n",
        "    # Return the predicted values\n",
        "    if normalize == True:\n",
        "        return tscaler.inverse_transform(t_pred[:, None]).reshape(-1)\n",
        "    else:\n",
        "        return t_pred[:, None].reshape(-1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49c10bea",
      "metadata": {},
      "source": [
        "````{admonition} Coding in python\n",
        "Different packages will offer distinct functionalities to perform the normalization. In the case of [`scikit-learn`](https://scikit-learn.org/stable/), the package [`sklearn.preprocessing`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) offers an wide range of normalizers:\n",
        "\n",
        "- `sklearn.preprocessing.StandardScaler` implements the normalization scheme from our linear models page, based on sample mean and standard deviation;\n",
        "- `sklearn.preprocessing.MinMaxScaler` uses dataset bounds to scale each feature based on its minimum and maximum value across the complete training dataset. This makes each separate input feature fall within a $[0,1]$ interval;\n",
        "- `sklearn.preprocessing.MaxAbsScaler` scales each feature based only on their maximum value across the dataset.\n",
        "- `sklearn.preprocessing.RobustScaler` implements a specialized normalization scheme that makes the trained models less sensitive to the presence of outliers in the training dataset\n",
        "\n",
        "Different normalizers are more suitable for different applications. It is usual to experiment with different normalizers when looking at a new problem. Regardless, using normalizers is a straightforward affair:\n",
        "\n",
        "```{python}\n",
        "# training dataset stored in `X`\n",
        "# scaler object stored in `scaler`\n",
        "\n",
        "scaler.fit(X)\n",
        "\n",
        "X_normalized = scaler.transform(X)\n",
        "```\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d328ca8-b2c1-4a95-adb5-6c198aff75f7",
      "metadata": {},
      "source": [
        "### Generalized linear models\n",
        "\n",
        "Let us first train the model above on our nonlinear dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e194c97",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "x_pred = np.linspace(-1, 2 * np.pi + 1, 1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dee4f17-3283-45ba-8b29-eed2f1691b38",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Let's run our model with linear basis funcitons and plot the results\n",
        "plot = magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=LinearBasis,\n",
        "    pred_label=\"Prediction $y(x)$\",\n",
        "    height=4.5,\n",
        ")\n",
        "plot.fig.canvas.toolbar_visible = False\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "612442a5",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig4.png\n",
        "Linear prediction\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab970576-5872-481b-8cb9-e00158aa2255",
      "metadata": {},
      "source": [
        "It is clear from the plot that a linear model with linear features lacks the flexibility to fit the data well. A bias-variance decomposition analysis for this model would show that it has little variance but shows a strong bias. We now consider nonlinear functions of the input $x$ as features/regressors to increase the flexibility of our linear model. A common approach is to use a set of polynomial basis functions,\n",
        "\n",
        "$$\n",
        "\\phi_j(x) = x^j,\n",
        "$$\n",
        "\n",
        "but numerous other choices are possible. The full formulation for a model with $M$ polynomial basis functions is thus\n",
        "\n",
        "$$\n",
        "y(x,\\mathbf{w}) = w_0 x^0 + w_1 x^1 + w_2 x^2 + ... + w_M x^M,\n",
        "$$\n",
        "\n",
        "which shows how the model is still linear w.r.t. $\\mathbf{w}$, even though it is no longer linear in the input parameters. The design matrix for this more general case reads\n",
        "\n",
        "$$\n",
        "\\Phi_{ij} = \\phi_j(x_i).\n",
        "$$\n",
        "\n",
        "As was the case for $\\mathbf{X}$, we need to ensure that $\\boldsymbol \\Phi$ has full column rank. This is not always the case the case, for example if we have more basis functions that data points, or if our basis functions are not linearly independent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8aeb098b-8592-42d2-9a22-4e4c4085af32",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# Here is a function for the polynomial basis functions:\n",
        "def PolynomialBasis(x, degree, **kwargs):  # **kwargs):\n",
        "    \"\"\"\n",
        "    A function that computes polynomial basis functions.\n",
        "\n",
        "    Arguments:\n",
        "    x       -  The datapoints\n",
        "    degree  -  The degree of the polynomial\n",
        "    \"\"\"\n",
        "    return np.array([x**i for i in range(degree + 1)]).transpose()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42a44a00-0adb-42e2-98fa-8bac52777796",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Let's test our implementation and visualize the polynomial basis functions\n",
        "degree = 5\n",
        "\n",
        "x_test = np.linspace(-1, 1, 100)\n",
        "Phi_p_test = PolynomialBasis(x_test, degree=degree)[:, 1::]\n",
        "\n",
        "# Plot the data and the ground truth\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "fig.canvas.toolbar_visible = False\n",
        "ax.set_position([0.2, 0.1, 0.7, 0.8])\n",
        "\n",
        "for i, row in enumerate(Phi_p_test.transpose()):\n",
        "    plt.plot(x_test, row, label=r\"$\\phi_{}(x)$\".format(i + 1))\n",
        "\n",
        "plt.xlabel(r\"$x$\")\n",
        "plt.ylabel(r\"$\\phi(x)$\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "833133e5",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig5.png\n",
        "Polynomial basis functions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98eb1dc1-c438-43e4-97df-3a4de2ee41cb",
      "metadata": {},
      "source": [
        "We obtain the linear model with nonlinear basis functions by replacing the coordinate vector $x$ with the  feature vector $\\boldsymbol{\\phi}(x)$\n",
        "\n",
        "$$\n",
        "y(x,\\mathbf{w}) = \\sum_{j=0}^M w_j \\phi_j(x) = \\mathbf{w}^T \\boldsymbol{\\phi} (x).\n",
        "$$\n",
        "\n",
        "The solution procedure remains the same, and we can solve for $\\bar{\\mathbf{w}}$ directly\n",
        "\n",
        "$$\n",
        "\\bar{\\mathbf{w}} = \\left(\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} \\right)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{t}.\n",
        "$$\n",
        "\n",
        "Let's take a look at the linear model with polynomial regressors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e3ebee9-d4e7-49b3-9b37-48b033cf16cc",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Plot the resulting predictions\n",
        "fig, ax = plt.subplots(2, 2, figsize=(9, 6), sharex=\"all\", sharey=\"all\")\n",
        "fig.canvas.toolbar_visible = False\n",
        "#plt.suptitle(r\"generalized linear regression for polynomials of degree $p$\")\n",
        "\n",
        "# Plot for degree=2\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=PolynomialBasis,\n",
        "    degree=2,\n",
        "    ax=ax[0][0],\n",
        "    hide_legend=True,\n",
        "    pred_label=r\"Prediction $y(x)$\",\n",
        "    title=r\"$degree={degree}$\",\n",
        ")\n",
        "\n",
        "# Plot for degree=5\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=PolynomialBasis,\n",
        "    degree=5,\n",
        "    ax=ax[0][1],\n",
        "    hide_legend=True,\n",
        "    title=r\"$degree={degree}$\",\n",
        ")\n",
        "\n",
        "# Plot for degree=10\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=PolynomialBasis,\n",
        "    degree=10,\n",
        "    ax=ax[1][0],\n",
        "    hide_legend=True,\n",
        "    title=r\"$degree={degree}$\",\n",
        ")\n",
        "\n",
        "# Plot for degree=25\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=PolynomialBasis,\n",
        "    degree=25,\n",
        "    ax=ax[1][1],\n",
        "    hide_legend=True,\n",
        "    title=r\"$degree={degree}$\",\n",
        ")\n",
        "\n",
        "# Add a general legend at the bottom of the plot\n",
        "plt.subplots_adjust(bottom=0.2)\n",
        "handles, labels = ax[0][0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"lower center\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "240c38f6",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig6.png\n",
        "Generalized linear regression for polynomials of degree $p$\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "419f96d8-362d-4aa9-a3b6-3c461777a564",
      "metadata": {},
      "source": [
        "That is looking much better already. However, the quality of the fit varies significantly with the degree of the polynomial basis. There seems to be an ideal model complexity for this specific problem. Try out the interactive tool below to get an idea of the interplay of the following variables:\n",
        "- $p$, the degree of the polynomial basis\n",
        "- $N$, the size of the training data set\n",
        "- $freq$, the frequency of the underlying truth\n",
        "- $\\varepsilon$, the level of noise associated with the data\n",
        "- The seed can be updated to generate new random data sets\n",
        "- The truth can be hidden to simulate a situation that is closer to a practical setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ad45efb-8e4c-4eb2-969d-321c660acdd4",
      "metadata": {
        "editable": true,
        "slideshow": {
          "slide_type": ""
        },
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "plot1 = magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=PolynomialBasis,\n",
        "    pred_label=r\"Prediction $y(x)$, $p={degree}$\",\n",
        ")\n",
        "plot1.fig.canvas.toolbar_visible = False\n",
        "plot1.add_sliders(\"epsilon\", \"degree\", \"N\", \"freq\")\n",
        "plot1.add_buttons(\"truth\", \"seed\", \"reset\")\n",
        "plot1.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a6bebc7-eec8-49de-acee-bb5152b27b8d",
      "metadata": {},
      "source": [
        "A few questions that might have crossed your mind when playing with the tool:\n",
        "\n",
        "- With a small amount of data ($N \\leq 11$), what happens if we have as many data points as parameters? $(p + 1 = N)$\n",
        "\n",
        "````{admonition} Answer\n",
        ":class: dropdown\n",
        "The model fits all data points exactly, since the least-squares problem is neither over- nor underdetermined. Note how the resulting model looks nothing like the ground truth, and how interpolating between data points usually gives low-quality predictions. These are signs of overfitting. Try it out for different noise levels to see something interesting!\n",
        "````\n",
        "\n",
        "- With a small amount of data ($N \\leq 11$), what happens if we have more model parameters than data? $(p + 1 > N)$\n",
        "\n",
        "````{admonition} Answer\n",
        ":class: dropdown\n",
        "The system is now underdetermined and therefore has no unique solution. The least-squares problem is also nearly singular and prone to numerical issues.\n",
        "````\n",
        "\n",
        "- We only have access to data in the interval $[0,2\\pi]$. How well does our model extrapolate beyond the data range?\n",
        "\n",
        "````{admonition} Answer\n",
        ":class: dropdown\n",
        "Unsurprisingly, higher-order models do not extrapolate well at all. In general it is a bad idea to use machine learning models in extrapolation. But try to increase the dataset size and select a more parsimonious model with lower $p$ and see what happens in extrapolation. There is still a lot we can do to carefully craft models that at the very least do not unreasonably deviate from the trends on the dataset when extrapolating.\n",
        "````"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2275baf1-1755-4c4b-8d6f-9fa96c8b8149",
      "metadata": {},
      "source": [
        "### Other choices of basis functions\n",
        "\n",
        "As mentioned previously, the polynomial basis is just one choice among many to define our model. Depending on the problem setting, a different set of basis functions might lead to better results. Another popular choice is the radial basis functions (also called Gaussian basis functions), given by\n",
        "\n",
        "$$\n",
        "\\phi_j(x) = \\exp\\left\\{-\\frac{(x-\\mu_j)^2}{2\\ell^2}\\right\\} \\quad \\mathrm{for} \\quad j=1,\\dots,M\n",
        "$$\n",
        "\n",
        "where $\\phi_j$ is centered around $\\mu_j$, $l$ determines the width, and $M$ refers to the number of basis functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30400a7c-7cd1-44b5-a16e-c3b65821efd3",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Here is a function for the RadialBasisFunctions:\n",
        "def RadialBasisFunctions(x, M_radial, l_radial, **kwargs):\n",
        "    \"\"\"\n",
        "    A function that computes radial basis functions.\n",
        "\n",
        "    Arguments:\n",
        "    x        -  The datapoints\n",
        "    M_radial -  The number of basis functions\n",
        "    l_radial -  The width of each basis function\n",
        "    \"\"\"\n",
        "    mu = np.linspace(-2, 2, M_radial)\n",
        "    num_basis = mu.shape[0]\n",
        "\n",
        "    Phi = np.ndarray((x.shape[0], num_basis))\n",
        "    for i in range(num_basis):\n",
        "        Phi[:, i] = np.exp(-0.5 * (x - mu[i]) ** 2 / l_radial**2)\n",
        "\n",
        "    return Phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccab4abd-ee85-4d29-b2e5-4e17c175a069",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Let's test our implementation\n",
        "l_radial = 0.5\n",
        "M_radial = 9\n",
        "\n",
        "x_test = np.linspace(-2, 2, 200)\n",
        "\n",
        "Phi_radial_test = RadialBasisFunctions(x_test, M_radial=M_radial, l_radial=l_radial)\n",
        "\n",
        "# Plot the data and the ground truth\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "fig.canvas.toolbar_visible = False\n",
        "ax.set_position([0.2, 0.1, 0.7, 0.8])\n",
        "\n",
        "for i, row in enumerate(Phi_radial_test.transpose()):\n",
        "    plt.plot(x_test, row, label=r\"$\\phi_{}(x)$\".format(i + 1))\n",
        "\n",
        "plt.xlabel(r\"$x$\")\n",
        "plt.ylabel(r\"$\\phi(x)$\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81748daa",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig8.png\n",
        "Radial basis functions\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "68ec8854-5eff-41c0-8be9-c7d424fcf578",
      "metadata": {},
      "source": [
        "One of the attributes of this model is the locality of its individual functions. This means data in one part of the domain will not impact predictions in other parts of the domain. Periodicity can be achieved with a Fourier basis. Wavelets are popular in signal processing since they are localized in both frequency and space. **It is up to the user to determine which basis function properties are desired for a given problem**, and this is an important part of model selection.\n",
        "\n",
        "Let's see how well the linear model with radial basis functions performs on the sine wave problem. Keep in mind that the lengthscale parameter corresponds to the lengthscale in the standardized space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a9cd800-de32-4eb6-b1e8-541788c8050e",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Plot the resulting predictions\n",
        "fig, ax = plt.subplots(2, 2, figsize=(9, 6), sharex=\"all\", sharey=\"all\")\n",
        "fig.canvas.toolbar_visible = False\n",
        "#plt.suptitle(\n",
        "#    r\"generalized linear regression for radial basis functions with varying $M$ and $l$\"\n",
        "#)\n",
        "\n",
        "# Plot for l=0.5, M=5\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=RadialBasisFunctions,\n",
        "    M_radial=5,\n",
        "    l_radial=0.5,\n",
        "    ax=ax[0][0],\n",
        "    hide_legend=True,\n",
        "    pred_label=r\"Prediction $y(x)$\",\n",
        "    title=r\"$l = {l_radial}, M = {M_radial}$\",\n",
        ")\n",
        "\n",
        "# Plot for l=0.5, M=15\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=RadialBasisFunctions,\n",
        "    M_radial=15,\n",
        "    l_radial=0.5,\n",
        "    ax=ax[0][1],\n",
        "    hide_legend=True,\n",
        "    title=r\"$l = {l_radial}, M = {M_radial}$\",\n",
        ")\n",
        "\n",
        "# Plot for l=1.5, M=5\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=RadialBasisFunctions,\n",
        "    M_radial=5,\n",
        "    l_radial=1.5,\n",
        "    ax=ax[1][0],\n",
        "    hide_legend=True,\n",
        "    title=r\"$l = {l_radial}, M = {M_radial}$\",\n",
        ")\n",
        "\n",
        "# Plot for l=1.5, M=15\n",
        "magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_pred,\n",
        "    basis=RadialBasisFunctions,\n",
        "    M_radial=15,\n",
        "    l_radial=1.5,\n",
        "    ax=ax[1][1],\n",
        "    hide_legend=True,\n",
        "    title=r\"$l = {l_radial}, M = {M_radial}$\",\n",
        ")\n",
        "\n",
        "# Add a general legend at the bottom of the plot\n",
        "plt.subplots_adjust(bottom=0.2)\n",
        "handles, labels = ax[0][0].get_legend_handles_labels()\n",
        "fig.legend(handles, labels, loc=\"lower center\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f1b9db3",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig9.png\n",
        "Generalized linear regression for radial basis functions with varying $M$ and $l$\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a764a91a",
      "metadata": {},
      "source": [
        "The figure above shows four different combinations of the hyperparameters (number of basis functions and length scale). The quality of the fit depends strongly on the parameter setting, but a visual inspection indicates our model can replicate the general trend."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c44517eb-601a-41fb-ad5a-4ea272357d76",
      "metadata": {},
      "source": [
        "## Regularization \n",
        "\n",
        "We've seen that the least squares approach can lead to severe over-fitting if complex models are trained on small datasets. Our strategy of limiting the number of basis functions a priori to counter overfitting puts us at risk of missing critical trends in the data. Another way to control model flexibility is by introducing a regularization term. This additional term essentially puts a penalty on the weights and prevents them from taking too large values unless supported by the data. The concept of regularized least squares will be demonstrated on the usual sine function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56431eab",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# The true function relating t to x\n",
        "def f_truth(x, freq=1, **kwargs):\n",
        "    # Return a sine with a frequency of f\n",
        "    return np.sin(x * freq)\n",
        "\n",
        "\n",
        "# The data generation function\n",
        "def f_data(epsilon=0.5, N=20, **kwargs):\n",
        "    # Apply a seed if one is given\n",
        "    if \"seed\" in kwargs:\n",
        "        np.random.seed(kwargs[\"seed\"])\n",
        "\n",
        "    # Get the minimum and maximum\n",
        "    xmin = kwargs.get(\"xmin\", 0)\n",
        "    xmax = kwargs.get(\"xmax\", 2 * np.pi)\n",
        "\n",
        "    # Generate N evenly spaced observation locations\n",
        "    x = np.linspace(xmin, xmax, N)\n",
        "\n",
        "    # Generate N noisy observations (1 at each location)\n",
        "    t = f_truth(x, **kwargs) + np.random.normal(0, epsilon, N)\n",
        "\n",
        "    # Return both the locations and the observations\n",
        "    return x, t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "067a704f",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Get the observed data\n",
        "x, t = f_data()\n",
        "x_pred = np.linspace(0, 2 * np.pi, 1000)\n",
        "\n",
        "# Plot the data and the ground truth\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "fig.canvas.toolbar_visible = False\n",
        "ax.set_position([0.2, 0.1, 0.7, 0.8])\n",
        "plt.plot(x_pred, f_truth(x_pred), \"k-\", label=r\"Ground truth $f(x)$\")\n",
        "plt.plot(x, t, \"x\", label=r\"Noisy data $(x,t)$\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"t\")\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n",
        "# plt.canvas.draw_idle()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f04d1e2",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig12.png\n",
        "Noisy data generated from a ground-truth sine wave\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06fe39df",
      "metadata": {},
      "source": [
        "We extend the data-dependent error $E_D(\\bf{w})$ with the regularization term $E_W(\\bf{w})$:\n",
        "\n",
        "$$\n",
        "E (\\mathbf{w}) = E_D (\\mathbf{w}) + \\lambda E_W (\\mathbf{w})\n",
        "$$\n",
        "\n",
        "with regularization parameter $\\lambda$ that controls the relative importance of two terms comprising the error function. A common choice for the regularizer is the sum-of-squares:\n",
        "\n",
        "$$\n",
        "E_W(\\mathbf{w}) = \\frac{1}{2} \\mathbf{w}^T \\mathbf{w}.\n",
        "$$\n",
        "\n",
        "The resulting model is known as L<sub>2</sub> regularization, ridge regression, or weight decay. The total error therefore becomes\n",
        "\n",
        "$$\n",
        "E (\\mathbf{w}) = \\frac{1}{2} \\sum_{n = 1}^{N} \\left( t_n - \\mathbf{w}^T \\boldsymbol{\\phi} (x_n) \\right)^2 + \\frac{\\lambda}{2} \\mathbf{w}^T \\mathbf{w}.\n",
        "$$\n",
        "\n",
        "<!-- Taking its gradient gives\n",
        "$$\n",
        "\\nabla_{\\mathbf{w}} E_{\\mathcal{D}} = \\frac{1}{N} \\sum_{n=1}^N \\left(t_n - \\mathbf{w}^T \\mathbf{x}_n \\right) \\mathbf{x}_n\n",
        "$$\n",
        "-->\n",
        "\n",
        "A useful property of the L<sub>2</sub> regularization is that the error functions is still a quadratic function of $\\mathbf{w}$, allowing for a closed form solution for its minimizer. Setting the gradient of the regularized error function with respect to $\\mathbf{w}$ to $0$ gives\n",
        "\n",
        "$$\n",
        "\\bar{\\mathbf{w}} = \\left(\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{I} \\right)^{-1} \\boldsymbol{\\Phi}^T \\mathbf{t}.\n",
        "$$\n",
        "\n",
        "<!-- Keeping in mind that $\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi}$ is a positive semi-definite matrix, the eigenvalues of the inverse of $\\boldsymbol{\\Phi}^T \\boldsymbol{\\Phi} + \\lambda \\mathbf{I}$ will shrink with an inceasing $\\lambda$, therefore pulling the weights towards $0$ compared to the unregularized solution. -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32b605ce",
      "metadata": {},
      "source": [
        "### Determining the regularization parameter\n",
        "\n",
        "We can use the formulation above in an interactive plot where you can control the dataset size, number of radial bases and the magnitude of the regularization parameter $\\lambda$:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac0a7242",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# Here is a function for the RadialBasisFunctions:\n",
        "def RadialBasisFunctions(x, M_radial, l_radial, **kwargs):\n",
        "    \"\"\"\n",
        "    A function that computes radial basis functions.\n",
        "\n",
        "    Arguments:\n",
        "    x        -  The datapoints\n",
        "    M_radial -  The number of basis functions\n",
        "    l_radial -  The width of each basis function\n",
        "    \"\"\"\n",
        "    mu = np.linspace(-2, 2, M_radial)\n",
        "    num_basis = mu.shape[0]\n",
        "\n",
        "    Phi = np.ndarray((x.shape[0], num_basis))\n",
        "    for i in range(num_basis):\n",
        "        Phi[:, i] = np.exp(-0.5 * (x - mu[i]) ** 2 / l_radial**2)\n",
        "\n",
        "    return Phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40c39fcf",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# Define a function that makes a prediction at the given locations, based on the given (x,t) data\n",
        "def predict(x, t, x_pred, basis, lam=None, **kwargs):\n",
        "    # reshape if necessary for scalers\n",
        "    x = x[:, None] if len(x.shape) == 1 else x\n",
        "    t = t[:, None] if len(t.shape) == 1 else t\n",
        "    x_pred = x_pred[:, None] if len(x_pred.shape) == 1 else x_pred\n",
        "\n",
        "    # normalize data\n",
        "    xscaler, tscaler = StandardScaler(), StandardScaler()\n",
        "    x_sc, t_sc = xscaler.fit_transform(x), tscaler.fit_transform(t)\n",
        "\n",
        "    # if 'M_radial' in kwargs:\n",
        "    #     Phi = basis (x_sc.reshape(-1), kwargs['M_radial'], kwargs['l_radial'], **kwargs)\n",
        "    # else:\n",
        "    Phi = basis(x_sc.reshape(-1), **kwargs)\n",
        "\n",
        "    # Get the variable matrix using the basis function phi\n",
        "    t_sc = t_sc.reshape(-1)\n",
        "    x_pred = xscaler.transform(x_pred).reshape(-1)\n",
        "\n",
        "    # Get identity matrix, set first entry to 0 to neglect bias in regularizaiton\n",
        "    I = np.identity(Phi.shape[1])\n",
        "    I[0, 0] = 0.0\n",
        "\n",
        "    # Get the coefficient vector\n",
        "    if lam is None:\n",
        "        w = np.linalg.solve(Phi.T @ Phi, Phi.T @ t_sc)\n",
        "    else:\n",
        "        w = np.linalg.solve(Phi.T @ Phi + lam * I, Phi.T @ t_sc)\n",
        "\n",
        "    # Make a prediction in the prediction locations\n",
        "    Phi_pred = basis(x_pred, **kwargs)\n",
        "    t_pred = Phi_pred @ w\n",
        "\n",
        "    # Return the predicted values\n",
        "    return tscaler.inverse_transform(t_pred[:, None]).reshape(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b96c7204",
      "metadata": {
        "tags": [
          "thebe-remove-input-init"
        ]
      },
      "outputs": [],
      "source": [
        "# # Let's try out our ridge regression model\n",
        "x_plot = np.linspace(0, 2 * np.pi, 1000)[:, None]\n",
        "\n",
        "# set seed and generate data\n",
        "np.random.seed(0)\n",
        "M_radial = 10\n",
        "l_radial = 0.5\n",
        "# x_train, t_train = f_data(N=15)\n",
        "\n",
        "# Let's take a look at our regularized solution\n",
        "plot = magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_plot,\n",
        "    basis=RadialBasisFunctions,\n",
        "    M_radial=M_radial,\n",
        "    l_radial=l_radial,\n",
        "    lam=1e-12,\n",
        "    pred_label=\"Prediction $y(x)$\",\n",
        "    height=4.5\n",
        ")\n",
        "plot.fig.canvas.toolbar_visible = False\n",
        "plot.add_slider(\"lam\",valmin=0,valmax=1,valinit=0,valfmt=None,orientation='horizontal',label=rf\"Reg. param. $\\lambda$\",update='pred')\n",
        "plot.add_slider(\"N\",valmax=100,valinit=10,valmin=10,label='Dataset ($N$)')\n",
        "plot.add_slider(\"M_radial\",valmin=2,label='Num. basis ($M$)',valmax=20,valinit=20)\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7817ff0",
      "metadata": {},
      "source": [
        "Play around with the number of radial basis functions and the regularization parameter. Try to answer the following questions:\n",
        "\n",
        "- What happens for a low number of RBF, and what happens for a high number of RBF?\n",
        "\n",
        "````{admonition} Answer\n",
        ":class: dropdown\n",
        "As you might expect, more radial basis functions means a more complex model, and therefore more prone to overfitting.\n",
        "````\n",
        "\n",
        "- How does this affect the value of $\\lambda$ that yields the (visually) best fit?\n",
        "\n",
        "````{admonition} Answer\n",
        ":class: dropdown\n",
        "Models with high variance will generally require more regularization to reach the same level of bias. So increasing the number of RBFs should require a higher value of $\\lambda$ in order to reach the same level of model complexity.\n",
        "````\n",
        "\n",
        "- How does a larger dataset affect the optimal regularization parameter $\\bar{\\lambda}$?\n",
        "\n",
        "````{admonition} Answer\n",
        ":class: dropdown\n",
        "As we increase $N$ and therefore let the model see more data, it becomes more tolerant to a higher range of values of $\\lambda$. That is, even very low values of $\\lambda$ will not lead to overfitting, as the data is now there to naturally limit model complexity. Again, overfitting is a problem which is much worse when we do not have a lot of data.\n",
        "````\n",
        "\n",
        "The pressing question is, of course: **how do we determine $\\lambda$?** \n",
        "\n",
        "The problem of controlling model complexity has been shifted from choosing a suitable set of basis functions to determining the regularization parameter $\\lambda$. **Optimizing for both $\\mathbf{w}$ and $\\lambda$ over the training dataset will always favor flexibility, leading to the trivial solution $\\lambda = 0$** and, therefore, to the unregularized least squares problem. Instead, similar to what we did before, the data should be partitioned into a training set and a validation set. The training set is used to determine the parameters $\\mathbf{w}$, and the validation set to optimize the model complexity through $\\lambda$. \n",
        "\n",
        "For the visualization below we use a pragmatic approach and sample another dense validation set from our ground truth instead of splitting the original training dataset. We then loop over several values of $\\lambda$ until we find the one corresponding to the minimum validation error. This is then our selected model. This leads to a best fit for $\\lambda = 0.048301 $ with $ \\text{MSE} = 0.267945$ "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd5088ea",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# set seed and generate data\n",
        "np.random.seed(4)\n",
        "x_train, t_train = f_data(N=15)\n",
        "x_val, t_val = f_data(N=500)\n",
        "\n",
        "# set up some vectores and params for the training\n",
        "lams = np.logspace(-10, 20, 100, base=np.exp(1))\n",
        "mse_val = np.zeros_like(lams)\n",
        "x_plot = np.linspace(0, 2 * np.pi, 1000)[:, None]\n",
        "\n",
        "M_radial = 10\n",
        "l_radial = 0.5\n",
        "\n",
        "# loop over all lambdas\n",
        "for i, lam in enumerate(lams):\n",
        "    t_pred = predict(\n",
        "        x_train,\n",
        "        t_train,\n",
        "        x_val,\n",
        "        RadialBasisFunctions,\n",
        "        M_radial=M_radial,\n",
        "        l_radial=l_radial,\n",
        "        lam=lam,\n",
        "    )\n",
        "    mse_val[i] = sum((t_val - t_pred) ** 2) / len(x_val)\n",
        "\n",
        "# find lambda with minimum mse\n",
        "loc = np.argmin(mse_val)\n",
        "lam_min = lams[loc]\n",
        "mse_min = mse_val[loc]\n",
        "#print(\"Best fit for lambda = {:e}  with MSE = {:4f}\".format(lam_min, mse_min))\n",
        "\n",
        "# plot mse over ln(lambda)\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "fig.canvas.toolbar_visible = False\n",
        "ax.set_position([0.2, 0.1, 0.7, 0.8])\n",
        "ax.plot(lams, mse_val)\n",
        "ax.set_xlabel(r\"$\\lambda$\")\n",
        "ax.set_ylabel(r\"MSE\")\n",
        "ax.set_xscale(\"log\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0049557a",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig15.png\n",
        "Minimum validation error for multiple values of $\\lambda$ \n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d0f271c",
      "metadata": {},
      "source": [
        "We then take a look at how the optimally regularized model looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e095bdb",
      "metadata": {
        "tags": [
          "remove-cell"
        ]
      },
      "outputs": [],
      "source": [
        "# Let's take a look at our regularized solution\n",
        "plot = magicplotter(\n",
        "    f_data,\n",
        "    f_truth,\n",
        "    predict,\n",
        "    x_plot,\n",
        "    x_train,\n",
        "    t_train,\n",
        "    basis=RadialBasisFunctions,\n",
        "    M_radial=M_radial,\n",
        "    l_radial=l_radial,\n",
        "    lam=lam_min,\n",
        "    pred_label=\"Prediction $y(x)$\",\n",
        "    height=4.5,\n",
        ")\n",
        "plot.fig.canvas.toolbar_visible = False\n",
        "plot.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d92d28a",
      "metadata": {},
      "source": [
        "```{figure} https://files.mude.citg.tudelft.nl/fig16.png\n",
        "Optimally regularized model\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fea33b40",
      "metadata": {},
      "source": [
        "That looks much better than the solution of the unregularized problem. Luckily, this problem has a unique solution $\\bar{\\lambda}$. Can you explain why the error first decreases but keeps growing again at some point? Do you always expect this non-monotonic characteristic when looking at the **training** error as a function of the regularization parameter?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09613782",
      "metadata": {},
      "source": [
        "% START-CREDIT\n",
        "% source: machine_learning\n",
        "```{attributiongrey} Attribution\n",
        ":class: attribution\n",
        "This chapter is written by Iuri Rocha, Anne Poot, Joep Storm and Leon Riccius. {ref}`Find out more here <machine_learning_credit>`.\n",
        "```\n",
        "% END-CREDIT"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
